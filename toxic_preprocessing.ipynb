{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "toxic_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kihongmin/DL/blob/master/toxic_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Iv75qyIvSiC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "https://arxiv.org/pdf/1805.12307.pdf 적용 예정.\n",
        "2020.04.21 -> preprocessing까지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4epwTeyCPDs5",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F8V-9kCvSiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dIsNzzi6QB4p",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train.csv.zip',compression='zip')\n",
        "test = pd.read_csv('test.csv.zip', compression='zip')\n",
        "test_label = pd.read_csv('test_labels.csv.zip',compression='zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G5cCStTQYPhn",
        "colab": {}
      },
      "source": [
        "y = train.iloc[:,2:].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2qMx7m4vSi3",
        "colab_type": "code",
        "colab": {},
        "outputId": "d96df948-ce57-4adc-fd2e-bd78d828cc2c"
      },
      "source": [
        "train.iloc[:,2:].sum(axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "toxic            15294\n",
              "severe_toxic      1595\n",
              "obscene           8449\n",
              "threat             478\n",
              "insult            7877\n",
              "identity_hate     1405\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsnKhFCZvSi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "stop_eng = stopwords.words('english')\n",
        "\n",
        "class makeData:\n",
        "    def __init__(self, maxlen, embed_dim, minCount, mode='embedLayer'):\n",
        "        self.maxlen = maxlen\n",
        "        self.embed_dim = embed_dim\n",
        "        self.minCount = minCount\n",
        "        if mode not in ['embedLayer','FastText']:\n",
        "            raise ValueError(\"Can not understand %s mode\"%(mode))\n",
        "        self.mode = mode\n",
        "        self.preprocessed_train_title = 'preprocessed_train_'+str(maxlen)+'_'+str(embed_dim)+'.pkl'\n",
        "        self.tokenized_train_title = 'tokenized_train_'+str(maxlen)+'_'+str(embed_dim)+'.pkl'\n",
        "        self.preprocessed_test_title = 'preprocessed_test_'+str(maxlen)+'_'+str(embed_dim)+'.pkl'\n",
        "        self.tokenized_test_title = 'tokenized_test_'+str(maxlen)+'_'+str(embed_dim)+'.pkl'\n",
        "        self.word_list_name = 'word_list_'+mode+'.pkl'\n",
        "        \n",
        "    def fit(self,train):\n",
        "        #전처리 끝난 데이터 있나요?\n",
        "        if self.preprocessed_train_title in os.listdir():\n",
        "            with open(self.preprocessed_train_title, 'rb') as f:\n",
        "                preprocessed_train =  pickle.load(f)\n",
        "            with open(self.word_list_name, 'rb') as f:\n",
        "                self.word_list =  pickle.load(f)\n",
        "            return preprocessed_train\n",
        "        else:\n",
        "            #토큰화 끝난 데이터 있나요?\n",
        "            if self.tokenized_train_title in os.listdir():\n",
        "                with open(self.tokenized_train_title, 'rb') as f:\n",
        "                    train =  pickle.load(f)\n",
        "            else:\n",
        "                train = train['comment_text'].apply(lambda x:x.lower())\n",
        "                print('apply word_tokenize')\n",
        "                train = train.apply(word_tokenize)\n",
        "                with open(self.tokenized_train_title, 'wb') as f:  \n",
        "                    pickle.dump(train, f)\n",
        "            #토큰화 종료 -> 전처리 시작\n",
        "            print('start to preprocess raw data')\n",
        "            train = self.preprocess(train,True)\n",
        "            with open(self.preprocessed_train_title, 'wb') as f: \n",
        "                pickle.dump(train, f) \n",
        "            return train\n",
        "        \n",
        "    def predict(self, test):\n",
        "        #전처리 끝난 데이터 있나요?\n",
        "        if self.preprocessed_test_title in os.listdir():\n",
        "            with open(self.preprocessed_test_title, 'rb') as f:\n",
        "                preprocessed_test =  pickle.load(f)\n",
        "            return preprocessed_test\n",
        "        else:\n",
        "            #토큰화 끝난 데이터 있나요?\n",
        "            if self.tokenized_test_title in os.listdir():\n",
        "                with open(self.tokenized_test_title, 'rb') as f:\n",
        "                    test = pickle.load(f)\n",
        "            else:\n",
        "                test = test['comment_text'].apply(lambda x:x.lower())\n",
        "                print('apply word_tokenize')\n",
        "                test = test.apply(word_tokenize)\n",
        "                with open(self.tokenized_test_title, 'wb') as f:  \n",
        "                    pickle.dump(train, f)\n",
        "            #토큰화 종료 -> 전처리 시작\n",
        "            print('start to preprocess raw data')\n",
        "            test = self.preprocess(test,False)\n",
        "            with open(self.preprocessed_test_title, 'wb') as f: \n",
        "                pickle.dump(test, f) \n",
        "            return test\n",
        "            \n",
        "        \n",
        "    def preprocess(self, df, is_train):\n",
        "        p = re.compile('[A-Za-z0-9]+')\n",
        "        df = df.apply(lambda x: list(filter(lambda y: y not in stop_eng, x)))\n",
        "        df = df.apply(lambda x: list(filter(lambda y: p.match(y),x)))\n",
        "        #train의 경우 word_list 생성 필요\n",
        "        if is_train :            \n",
        "        #모델에 임베딩 레이어 사용할 때\n",
        "            print('start to make valid word list')\n",
        "            if self.mode == 'embedLayer':\n",
        "                counter = []\n",
        "                for i in df:\n",
        "                    counter.extend(i)\n",
        "                counter = Counter(counter)\n",
        "                self.word_list = dict(list(filter(lambda x:x[1]>5, counter.items())))\n",
        "                with open(self.word_list_name, 'wb') as f:  \n",
        "                    pickle.dump(self.word_list, f)\n",
        "            #FastText로 임베딩된 워드 사용할 때\n",
        "            elif self.mode == 'FastText':\n",
        "                name = 'fasttext_'+str(self.maxlen)+'_'+str(self.embed_dim)+'.vec'\n",
        "                if name in os.listdir():\n",
        "                    self.ft_model = FastText.load(name)\n",
        "                else:\n",
        "                    self.ft_model = FastText(train, size=embed_dim, window=5, min_count=self.minCount, workers=4, sg=1)\n",
        "                    self.ft_model.save(name)\n",
        "                self.word_list = self.ft_model.wv.vocab.keys()\n",
        "                with open(self.word_list_name, 'wb') as f:  \n",
        "                    pickle.dump(self.word_list, f)\n",
        "        #----------\n",
        "        #word_list에 없는 단어들 버림\n",
        "        print('start to filter words')\n",
        "\n",
        "        for i, sentence in enumerate(df):\n",
        "            df[i] = list(filter(lambda x: x in self.word_list, sentence))\n",
        "        #이제는 mincount 이상의 빈도로 출현한 단어만 적용됨\n",
        "        #-----------\n",
        "        #문장의 단어 개수 통일\n",
        "        \n",
        "        print('start to fit shape')\n",
        "        if self.mode == 'embedLayer':            \n",
        "            tokenizer = Tokenizer()\n",
        "            tokenizer.fit_on_texts(df)\n",
        "            df = tokenizer.texts_to_sequences(df)\n",
        "            df = pad_sequences(df, maxlen=self.maxlen)\n",
        "            return df\n",
        "                    \n",
        "        elif self.mode == 'FastText':\n",
        "            outlayer = np.zeros((len(df),self.maxlen,self.embed_dim))\n",
        "            for i, sentence in enumerate(tqdm_notebook(df)):\n",
        "                temp = np.zeros((self.maxlen,self.embed_dim))\n",
        "                index = self.maxlen-len(sentence) if len(sentence) < self.maxlen else 0\n",
        "                for word in sentence:\n",
        "                    if index == self.maxlen:\n",
        "                        break\n",
        "                    temp[index] = self.ft_model.wv[word]\n",
        "                    index+=1\n",
        "                outlayer[i] = temp\n",
        "            return outlayer        "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}