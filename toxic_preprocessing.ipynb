{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "toxic_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kihongmin/DL/blob/master/toxic_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDCois_EYnu_",
        "colab_type": "text"
      },
      "source": [
        "## embed_dim 150이상은 램부족..  \n",
        "## complete 파일은 120_150 기준 22GB. 로컬에서 저장 비효율적"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4epwTeyCPDs5",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45LnwEq8YnvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dIsNzzi6QB4p",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train.csv.zip',compression='zip')\n",
        "test = pd.read_csv('test.csv.zip', compression='zip')\n",
        "test_label = pd.read_csv('test_labels.csv.zip',compression='zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G5cCStTQYPhn",
        "colab": {}
      },
      "source": [
        "y = train.iloc[:,2:].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7zwFviDYnvs",
        "colab_type": "code",
        "outputId": "155c7b36-ae5e-4b07-c2a4-8623c94764fd",
        "colab": {}
      },
      "source": [
        "train.iloc[:,2:].sum(axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "toxic            15294\n",
              "severe_toxic      1595\n",
              "obscene           8449\n",
              "threat             478\n",
              "insult            7877\n",
              "identity_hate     1405\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXR1yiQbYnwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import FastText\n",
        "from nltk.corpus import stopwords\n",
        "import pickle\n",
        "\n",
        "stop_eng = stopwords.words('english')\n",
        "def raw2input(raw,maxlen,embed_dim,dataClass):\n",
        "    inputTitle = 'complete_'+dataClass+str(maxlen)+'_'+str(embed_dim)+'.pkl'\n",
        "    preTitle = 'preprocessed_'+dataClass+'.pkl'\n",
        "    dataTitle = 'tokenized_'+ dataClass +'.pkl'\n",
        "    if inputTitle in os.listdir():\n",
        "        with open(inputTitle, 'rb') as f:\n",
        "            raw = pickle.load(f)\n",
        "        return raw\n",
        "    else:\n",
        "        if preTitle in os.listdir():\n",
        "            with open(preTitle, 'rb') as f:\n",
        "                raw = pickle.load(f)\n",
        "                print('complete load to preprocessed data')\n",
        "        else:\n",
        "            if dataTitle in os.listdir():\n",
        "                with open(dataTitle, 'rb') as f: \n",
        "                    raw = pickle.load(f)\n",
        "                print('complete to load data')\n",
        "            else:\n",
        "                raw = raw['comment_text'].apply(lambda x:x.lower())\n",
        "                print('apply word_tokenize')\n",
        "                raw = raw.apply(word_tokenize)\n",
        "                with open(dataTitle, 'wb') as f:  \n",
        "                    pickle.dump(raw, f)\n",
        "            print('start to preprocess raw data')\n",
        "            raw = preprocess(raw)\n",
        "            with open(preTitle, 'wb') as f: \n",
        "                pickle.dump(raw, f)        \n",
        "        raw = raw.to_numpy()\n",
        "        ft_model = build_embedding(raw,maxlen,embed_dim,dataClass)\n",
        "        for i,word_list in enumerate(raw):\n",
        "            raw[i] = np.array(list(filter(lambda x: x in ft_model.wv.vocab.keys(), word_list)))\n",
        "\n",
        "        df = np.zeros((len(raw),maxlen,embed_dim))\n",
        "        print('start to fit shape')\n",
        "        for i, word_list in enumerate(tqdm_notebook(raw)):\n",
        "            temp = np.zeros((maxlen,embed_dim))\n",
        "            index = maxlen-len(word_list) if len(word_list) < maxlen else 0\n",
        "            for word in word_list:\n",
        "                if index == maxlen:\n",
        "                    break\n",
        "                temp[index] = ft_model.wv[word]\n",
        "                index+=1\n",
        "            df[i] = temp\n",
        "#        너무 커서 저장 안함\n",
        "#        with open(inputTitle, 'wb') as f: \n",
        "#                pickle.dump(df, f,protocol=4)  \n",
        "        return df\n",
        "        \n",
        "def build_embedding(train,maxlen,embed_dim, dataClass):\n",
        "    name = 'fasttext_'+str(maxlen)+'_'+str(embed_dim)+'.vec'\n",
        "    if dataClass == 'test':\n",
        "        try:\n",
        "            ft_model = FastText.load(name)\n",
        "            return ft_model\n",
        "        except:\n",
        "            raise RuntimError('FastText model does not exist. Please make it by train data')\n",
        "    else:        \n",
        "        if name in os.listdir():\n",
        "            ft_model = FastText.load(name)\n",
        "            return ft_model\n",
        "        else:\n",
        "            print('make new FastText')\n",
        "            ft_model = FastText(train, size=embed_dim, window=5, min_count=5, workers=4, sg=1)\n",
        "            ft_model.save(name)\n",
        "            return ft_model\n",
        "    \n",
        "\n",
        "def preprocess(df):\n",
        "    df = df.apply(lambda x: list(filter(lambda y: y not in stop_eng, x)))\n",
        "    p = re.compile('[A-Za-z0-9]+')\n",
        "    df = df.apply(lambda x: list(filter(lambda y: p.match(y),x)))\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCBeGVKyYnwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen=120\n",
        "embed_dim = 150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1wDMcVQNYnwf",
        "colab_type": "code",
        "outputId": "763f14cf-1745-40de-dd13-bce1c66b0c36",
        "colab": {
          "referenced_widgets": [
            "7445fd41257f465ab083cf70eb20e869"
          ]
        }
      },
      "source": [
        "df_train = raw2input(train,maxlen,embed_dim,'train')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "complete load to preprocessed data\n",
            "start to fit shape\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:45: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7445fd41257f465ab083cf70eb20e869",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=159571.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "xtPiZZh6Ynwo",
        "colab_type": "code",
        "outputId": "6ef33bc7-15a1-42be-df7a-099210a0b70d",
        "colab": {
          "referenced_widgets": [
            "3aa23699c80a4bea9c30c284f17767b7"
          ]
        }
      },
      "source": [
        "df_test = raw2input(test,maxlen,embed_dim,'test')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "complete load to preprocessed data\n",
            "start to fit shape\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:45: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3aa23699c80a4bea9c30c284f17767b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=153164.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7U5MT8PYnws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "def predict_model(df_train,y,df_test):\n",
        "    inp = Input(shape=(maxlen,embed_dim, ))\n",
        "    x = LSTM(60, return_sequences=True,name='lstm_layer')(inp)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(200, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    batch_size = 256\n",
        "    epochs = 2\n",
        "    model.fit(df_train,y,batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "    model.save('model'+str(maxlen)+str(embed_dim)+'.h5')\n",
        "    t = model.predict(df_test, batch_size=batch_size)\n",
        "    sub_name = ''\n",
        "    sub = pd.read_csv('sample_submission.csv.zip',compression='zip')\n",
        "    subm = pd.DataFrame(t,columns=sub.columns[1:])\n",
        "    subm['id'] = sub['id']\n",
        "    subm.to_csv('sub4.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0KGKYxMzYnww",
        "colab_type": "code",
        "outputId": "5c78fe47-49d1-478c-ad3c-a3538faed70f",
        "colab": {}
      },
      "source": [
        "predict_model(df_train,y,df_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/2\n",
            "143613/143613 [==============================] - 1218s 8ms/step - loss: 0.0882 - accuracy: 0.9732 - val_loss: 0.0533 - val_accuracy: 0.9805\n",
            "Epoch 2/2\n",
            "143613/143613 [==============================] - 1174s 8ms/step - loss: 0.0520 - accuracy: 0.9810 - val_loss: 0.0529 - val_accuracy: 0.9805\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}