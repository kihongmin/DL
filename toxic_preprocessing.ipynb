{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "toxic_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kihongmin/DL/blob/master/toxic_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Iv75qyIvSiC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "https://arxiv.org/pdf/1805.12307.pdf 적용 예정.\n",
        "2020.04.21 -> preprocessing까지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJVY3KAY-eWV",
        "colab_type": "code",
        "outputId": "30224802-f76e-4f0c-d78b-c0fb76398e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4epwTeyCPDs5",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F8V-9kCvSiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4GMpr5w-vmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/ds/kaggle/toxic')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dIsNzzi6QB4p",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train.csv.zip',compression='zip')\n",
        "test = pd.read_csv('test.csv.zip', compression='zip')\n",
        "test_label = pd.read_csv('test_labels.csv.zip',compression='zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G5cCStTQYPhn",
        "colab": {}
      },
      "source": [
        "y = train.iloc[:,2:].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2qMx7m4vSi3",
        "colab_type": "code",
        "outputId": "e4c3a165-7a25-447a-dd3c-6c2934c570ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "train.iloc[:,2:].sum(axis=0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "toxic            15294\n",
              "severe_toxic      1595\n",
              "obscene           8449\n",
              "threat             478\n",
              "insult            7877\n",
              "identity_hate     1405\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRd7Z7Euophz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ad01af5f-b20e-42a9-fde1-64c65cf771be"
      },
      "source": [
        "  import nltk\n",
        "  nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsnKhFCZvSi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import os, pickle, re\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "stop_eng = stopwords.words('english')\n",
        "\n",
        "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "class makeData:\n",
        "    def __init__(self, maxlen, embed_dim, minCount, mode='embedLayer'):\n",
        "        self.maxlen = maxlen\n",
        "        self.embed_dim = embed_dim\n",
        "        self.minCount = minCount\n",
        "        if mode not in ['embedLayer','FastText']:\n",
        "            raise ValueError(\"Can not understand %s mode\"%(mode))\n",
        "        self.mode = mode\n",
        "        self.preprocessed_train_title = 'preprocessed_train_'+str(maxlen)+'_'+str(embed_dim)+'_'+str(minCount)+'.pkl'\n",
        "        self.tokenized_train_title = 'tokenized_train_'+str(maxlen)+'_'+str(embed_dim)+'_'+str(minCount)+'.pkl'\n",
        "        self.preprocessed_test_title = 'preprocessed_test_'+str(maxlen)+'_'+str(embed_dim)+'_'+str(minCount)+'.pkl'\n",
        "        self.tokenized_test_title = 'tokenized_test_'+str(maxlen)+'_'+str(embed_dim)+'_'+str(minCount)+'.pkl'\n",
        "        self.word_list_name = 'word_list_'+mode+'_'+str(minCount)+'.pkl'\n",
        "        self.word_list = None\n",
        "    \n",
        "    def run(self,train,test):\n",
        "        df_train = self.fit(train,True)\n",
        "        df_test = self.fit(test,False)\n",
        "        return df_train, df_test\n",
        "    \n",
        "    def fit(self,df,is_train=False):#문장들만 넣어주세요\n",
        "        #전처리 끝난 데이터 있나요?\n",
        "        if self.preprocessed_train_title in os.listdir():\n",
        "            df =load_data(self.preprocessed_train_title)\n",
        "            return df\n",
        "        else:\n",
        "            #토큰화 끝난 데이터 있나요?\n",
        "            if self.tokenized_train_title in os.listdir():\n",
        "                df = load_data(self.tokenized_train_title)\n",
        "            else:\n",
        "                #tokenizer fit\n",
        "                if is_train == True:\n",
        "                    #토큰화 + 특수문자제거\n",
        "                    tokenizer = Tokenizer()\n",
        "                    tokenizer.fit_on_texts(df)\n",
        "                    #-------------------------\n",
        "                    #tokenizer는 dict 기반으로 단어->숫자 변환\n",
        "                    #word_index의 값을 삭제해 mincount, 불용어 처리\n",
        "                    #-------------------------\n",
        "                    #불용어 처리\n",
        "                    for i in stop_eng:\n",
        "                        tokenizer.word_index.pop(i,None)                    \n",
        "                    #minCount이하의 출현->삭제목록\n",
        "                    lessCount = dict(filter(lambda x:x[1]<5,tokenizer.word_counts.items()))\n",
        "                    for w in lessCount.keys():\n",
        "                        tokenizer.word_index.pop(w,None)\n",
        "                    self.tokenizer = tokenizer               \n",
        "                #test데이터는 train 데이터를 통해 토큰화 목록에 있는 친구들만 토큰화    \n",
        "                df=self.tokenizer.texts_to_sequences(df)\n",
        "            #토큰화 종료 -> len 맞춤\n",
        "            df = pad_sequences(df, maxlen=self.maxlen)\n",
        "            return df        \n",
        "    #이전버전\n",
        "    '''\n",
        "    def preprocess(self, df, is_train):\n",
        "        if is_train :\n",
        "        #모델에 임베딩 레이어 사용할 때\n",
        "            if self.word_list is None:\n",
        "                print('start to make valid word list')\n",
        "                if self.mode == 'embedLayer':\n",
        "                    counter = []\n",
        "                    for i in df:\n",
        "                        counter.extend(i)\n",
        "                    counter = Counter(counter)\n",
        "                    self.word_list = dict(list(filter(lambda x:x[1]>self.minCount, counter.items())))\n",
        "                    save_data(self.word_list_name,self.word_list)\n",
        "\n",
        "                #FastText로 임베딩된 워드 사용할 때\n",
        "                ## 0422 pretrainned Fast text 사용\n",
        "                elif self.mode == 'FastText':\n",
        "                    #name = 'fasttext_'+str(self.maxlen)+'_'+str(self.embed_dim)+'.vec'\n",
        "                    name = 'crawl-300d-2M.vec'\n",
        "                    if name in os.listdir():\n",
        "                        print(\"apply %s\"%(name))\n",
        "                        #self.ft_model = FastText.load(name)\n",
        "                        self.word_list = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open('crawl-300d-2M.vec'))                    \n",
        "                    else:\n",
        "                        self.ft_model = FastText(train, size=embed_dim, window=5, min_count=self.minCount, workers=4, sg=1)\n",
        "                        self.ft_model.save(name)\n",
        "                        save_data(self.word_list_name,self.word_list)\n",
        "                        self.word_list = dict(self.ft_model.wv.vocab.items())\n",
        "        #----------\n",
        "        #word_list에 없는 단어들 버림\n",
        "        #1 epoch에 30분->10분 단축, accuracy 0.01상승\n",
        "        print('start to filter words')\n",
        "        for i, sentence in enumerate(df):\n",
        "            df[i] = list(filter(lambda x: x in self.word_list, sentence))\n",
        "        #이제는 mincount 이상의 빈도로 출현한 단어만 적용됨\n",
        "        #-----------\n",
        "        #문장의 단어 개수 통일        \n",
        "        print('start to fit shape')\n",
        "        if self.mode == 'embedLayer':\n",
        "            if is_train == True:\n",
        "                tokenizer = Tokenizer()\n",
        "                tokenizer.fit_on_texts(df)\n",
        "                self.el_tokenizer = tokenizer\n",
        "                \n",
        "                df = self.el_tokenizer.texts_to_sequences(df)\n",
        "                df = pad_sequences(df, maxlen=self.maxlen)\n",
        "                return df\n",
        "            else:                \n",
        "                df = self.el_tokenizer.texts_to_sequences(df)\n",
        "                df = pad_sequences(df, maxlen=self.maxlen)\n",
        "                return df\n",
        "        elif self.mode == 'FastText':\n",
        "            outlayer = np.zeros((len(df),self.maxlen,self.embed_dim))\n",
        "            for i, sentence in enumerate(tqdm_notebook(df)):\n",
        "                temp = np.zeros((self.maxlen,self.embed_dim))\n",
        "                index = self.maxlen-len(sentence) if len(sentence) < self.maxlen else 0\n",
        "                for word in sentence:\n",
        "                    if index == self.maxlen:\n",
        "                        break\n",
        "                    temp[index] = self.word_list[word]\n",
        "                    index+=1\n",
        "                outlayer[i] = temp\n",
        "            return outlayer        \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn-06MMi_aol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys,  re, csv, codecs\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
        "def predict_model_FastText(df_train,y,df_test):\n",
        "    inp = Input(shape=(maxlen,embed_dim, ))\n",
        "    x = LSTM(60, return_sequences=True,name='lstm_layer')(inp)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Dense(200, activation=\"relu\")(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    batch_size = 256\n",
        "    epochs = 2\n",
        "    model.fit(df_train,y,batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "    model.save('model'+str(maxlen)+str(embed_dim)+'.h5')\n",
        "    t = model.predict(df_test, batch_size=batch_size)\n",
        "    sub = pd.read_csv('sample_submission.csv.zip',compression='zip')\n",
        "    subm = pd.DataFrame(t,columns=sub.columns[1:])\n",
        "    subm['id'] = sub['id']\n",
        "    subm.to_csv('sub4.csv',index=False)\n",
        "    return model\n",
        "    \n",
        "def predict_model(df_train,y,df_test):\n",
        "    max_feature = max(list(map(lambda x: max(x),df_train)))*2\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_feature,embed_dim)(inp)\n",
        "    x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Dense(200, activation=\"relu\")(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    batch_size = 128\n",
        "    epochs = 2\n",
        "    model.fit(df_train,y,batch_size=batch_size, epochs=epochs, validation_split=0.1,)\n",
        "    model.save('model'+str(maxlen)+str(embed_dim)+'.h5')\n",
        "    t = model.predict(df_test, batch_size=batch_size)\n",
        "    sub = pd.read_csv('sample_submission.csv.zip',compression='zip')\n",
        "    subm = pd.DataFrame(t,columns=sub.columns[1:])\n",
        "    subm['id'] = sub['id']\n",
        "    \n",
        "    subm.to_csv('mySubmission.csv',index=False)\n",
        "    return model    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXN83gcQ_Ip8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen,embed_dim = 100, 300\n",
        "dataBuilder = makeData(maxlen, embed_dim, 5)\n",
        "\n",
        "df_train, df_test = dataBuilder.run(train['comment_text'],test['comment_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxECB9lxo279",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "467f0f55-414c-45fb-ab18-3e8fc2c86d25"
      },
      "source": [
        "model = predict_model(df_train,y,df_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1122/1122 [==============================] - 179s 160ms/step - loss: 0.0826 - accuracy: 0.9758 - val_loss: 0.0526 - val_accuracy: 0.9940\n",
            "Epoch 2/2\n",
            "1122/1122 [==============================] - 178s 159ms/step - loss: 0.0443 - accuracy: 0.9929 - val_loss: 0.0498 - val_accuracy: 0.9931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifNp5BsKo44L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}